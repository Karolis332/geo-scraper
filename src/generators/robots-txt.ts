/**
 * Generate enhanced robots.txt with AI crawler directives.
 * Covers 13 known AI crawler user-agents.
 * Preserves existing Disallow rules from the target site.
 */

import type { SiteCrawlResult, GeneratorOptions } from '../crawler/page-data.js';

interface CrawlerDirective {
  userAgent: string;
  company: string;
  purpose: string;
}

const AI_CRAWLERS: CrawlerDirective[] = [
  { userAgent: 'GPTBot', company: 'OpenAI', purpose: 'Model training' },
  { userAgent: 'OAI-SearchBot', company: 'OpenAI', purpose: 'Search results' },
  { userAgent: 'ChatGPT-User', company: 'OpenAI', purpose: 'Live browsing' },
  { userAgent: 'ClaudeBot', company: 'Anthropic', purpose: 'Model training & citations' },
  { userAgent: 'Claude-SearchBot', company: 'Anthropic', purpose: 'Search' },
  { userAgent: 'Google-Extended', company: 'Google', purpose: 'Gemini training' },
  { userAgent: 'Applebot-Extended', company: 'Apple', purpose: 'Apple Intelligence' },
  { userAgent: 'Meta-ExternalAgent', company: 'Meta', purpose: 'AI training' },
  { userAgent: 'PerplexityBot', company: 'Perplexity', purpose: 'Answer engine' },
  { userAgent: 'Amazonbot', company: 'Amazon', purpose: 'Alexa / AI' },
  { userAgent: 'CCBot', company: 'Common Crawl', purpose: 'Open dataset' },
  { userAgent: 'DuckAssistBot', company: 'DuckDuckGo', purpose: 'DuckDuckGo AI' },
  { userAgent: 'Bytespider', company: 'ByteDance', purpose: 'AI training' },
];

const AI_USER_AGENTS = new Set(AI_CRAWLERS.map(c => c.userAgent.toLowerCase()));
const STANDARD_SEARCH_AGENTS = new Set(['googlebot', 'bingbot', 'applebot']);

export function generateRobotsTxt(
  crawlResult: SiteCrawlResult,
  options: GeneratorOptions,
): string {
  const { baseUrl, existingGeoFiles } = crawlResult;
  const allowTraining = !options.denyTraining;
  const lines: string[] = [];

  // Parse existing robots.txt to preserve custom Disallow rules
  const existingRules = parseExistingRobotsTxt(existingGeoFiles.robotsTxt);

  // Header comment
  lines.push('# robots.txt — Generated by geo-scraper');
  lines.push(`# Site: ${baseUrl}`);
  lines.push(`# Generated: ${new Date().toISOString().split('T')[0]}`);
  lines.push('#');
  lines.push('# AI Crawler Directives');
  lines.push(`# Training data access: ${allowTraining ? 'ALLOWED' : 'DENIED'}`);
  if (existingRules.wildcardDisallows.length > 0) {
    lines.push('# NOTE: Existing Disallow rules from original robots.txt have been preserved');
  }
  lines.push('');

  // Standard web crawlers — always allow
  lines.push('# Standard Search Engines');
  for (const agent of ['Googlebot', 'Bingbot', 'Applebot']) {
    lines.push(`User-agent: ${agent}`);
    lines.push('Allow: /');
    // Preserve existing Disallow rules for this agent
    const agentRules = existingRules.agentDisallows.get(agent.toLowerCase());
    if (agentRules) {
      for (const rule of agentRules) {
        lines.push(`Disallow: ${rule}`);
      }
    }
    lines.push('');
  }

  // AI crawlers section
  lines.push('# ====================================');
  lines.push('# AI / LLM Crawler Directives');
  lines.push('# ====================================');
  lines.push('');

  // Group: Search / live browsing (always allow for visibility)
  const searchCrawlers = AI_CRAWLERS.filter(c =>
    c.purpose.includes('Search') || c.purpose.includes('browsing') || c.purpose.includes('Answer')
  );
  const trainingCrawlers = AI_CRAWLERS.filter(c =>
    !c.purpose.includes('Search') && !c.purpose.includes('browsing') && !c.purpose.includes('Answer')
  );

  lines.push('# AI Search & Retrieval (recommended: allow for visibility)');
  for (const crawler of searchCrawlers) {
    lines.push(`# ${crawler.company} — ${crawler.purpose}`);
    lines.push(`User-agent: ${crawler.userAgent}`);
    lines.push('Allow: /');
    // Include wildcard Disallow rules for search crawlers too
    for (const rule of existingRules.wildcardDisallows) {
      lines.push(`Disallow: ${rule}`);
    }
    lines.push('');
  }

  lines.push(`# AI Training Crawlers (${allowTraining ? 'allowed' : 'denied'})`);
  for (const crawler of trainingCrawlers) {
    lines.push(`# ${crawler.company} — ${crawler.purpose}`);
    lines.push(`User-agent: ${crawler.userAgent}`);
    if (allowTraining) {
      lines.push('Allow: /');
      for (const rule of existingRules.wildcardDisallows) {
        lines.push(`Disallow: ${rule}`);
      }
    } else {
      lines.push('Disallow: /');
    }
    lines.push(`Crawl-delay: 10`);
    lines.push('');
  }

  // Catch-all
  lines.push('# Default: allow all other crawlers');
  lines.push('User-agent: *');
  lines.push('Allow: /');
  for (const rule of existingRules.wildcardDisallows) {
    lines.push(`Disallow: ${rule}`);
  }
  lines.push('');

  // Sitemap reference
  lines.push(`Sitemap: ${baseUrl}/sitemap.xml`);
  lines.push('');

  // LLMs.txt hint (not standard, but some crawlers check)
  lines.push(`# LLMs.txt: ${baseUrl}/llms.txt`);
  lines.push('');

  return lines.join('\n');
}

interface ExistingRobotsParsed {
  /** Disallow paths from User-agent: * blocks */
  wildcardDisallows: string[];
  /** Disallow paths keyed by lowercased user-agent */
  agentDisallows: Map<string, string[]>;
}

function parseExistingRobotsTxt(content: string | null): ExistingRobotsParsed {
  const result: ExistingRobotsParsed = {
    wildcardDisallows: [],
    agentDisallows: new Map(),
  };

  if (!content) return result;

  let currentAgents: string[] = [];

  for (const rawLine of content.split('\n')) {
    const line = rawLine.trim();
    if (!line || line.startsWith('#')) continue;

    const uaMatch = line.match(/^User-agent:\s*(.+)/i);
    if (uaMatch) {
      currentAgents.push(uaMatch[1].trim().toLowerCase());
      continue;
    }

    const disallowMatch = line.match(/^Disallow:\s*(.+)/i);
    if (disallowMatch && currentAgents.length > 0) {
      const path = disallowMatch[1].trim();
      if (!path) continue;

      for (const agent of currentAgents) {
        // Skip AI crawler-specific rules — we regenerate those
        if (AI_USER_AGENTS.has(agent)) continue;

        if (agent === '*') {
          if (!result.wildcardDisallows.includes(path)) {
            result.wildcardDisallows.push(path);
          }
        } else {
          if (!result.agentDisallows.has(agent)) {
            result.agentDisallows.set(agent, []);
          }
          const paths = result.agentDisallows.get(agent)!;
          if (!paths.includes(path)) {
            paths.push(path);
          }
        }
      }
      continue;
    }

    // Any non-User-agent, non-Disallow line resets the current agent group
    if (!line.match(/^(Allow|Crawl-delay|Sitemap):/i)) {
      currentAgents = [];
    }
  }

  return result;
}
